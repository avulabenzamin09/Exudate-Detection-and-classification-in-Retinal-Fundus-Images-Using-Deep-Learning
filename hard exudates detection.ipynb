{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from resizeimage import resizeimage\n",
    "from PIL import Image\n",
    "from tensorflow.python.ops.image_ops_impl import ResizeMethod  \n",
    "#from pandas_ml import ConfusionMatrix\n",
    "from random import shuffle\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in range(0,200000,2):\n",
    "    a.append(0)\n",
    "    a.append(1)\n",
    "bl=np.zeros((200000,2))\n",
    "bl[np.arange(200000), a]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in range(0,100000,2):\n",
    "    a.append(0)\n",
    "    a.append(1)\n",
    "    \n",
    "test_label=np.zeros((100000,2))\n",
    "test_label[np.arange(100000), a]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape,na):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial,name=na)\n",
    "\n",
    "def bias_variable(shape,na):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=na)\n",
    "\n",
    "def conv2d(xe, W):\n",
    "    return tf.nn.conv2d(xe, W, strides=[ 1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def conv1d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "\n",
    "def max_pool(xe):\n",
    "    return tf.nn.max_pool(xe, ksize=[1, 3, 3, 1],strides=[1,2,2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_s2(xe):\n",
    "    return tf.nn.max_pool(xe, ksize=[1, 3,3, 1],strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn(is_training,inputs,decay=0.999):\n",
    "    scale=tf.Variable(tf.ones(inputs.get_shape()[-1:]))\n",
    "    beta = tf.Variable(tf.zeros(inputs.get_shape()[-1:]))\n",
    "    pop_mean = tf.Variable(tf.zeros(inputs.get_shape()[-1:]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones(inputs.get_shape()[-1:]), trainable=False)\n",
    "    mean = tf.cond(is_training,lambda:tf.nn.moments(inputs,[0,1,2])[0],lambda:tf.ones(tf.shape(inputs.get_shape()[-1:]))*pop_mean)\n",
    "    var = tf.cond(is_training,lambda:tf.nn.moments(inputs,[0,1,2])[1],lambda:tf.ones(tf.shape(inputs.get_shape()[-1:]))*pop_var)\n",
    "    train_mean = tf.cond(is_training,lambda:tf.assign(pop_mean,pop_mean * decay + mean * (1 - decay)),lambda:tf.ones([0]))\n",
    "    train_var= tf.cond(is_training,lambda:tf.assign(pop_var,pop_var * decay + var * (1 - decay)),lambda:tf.ones([0]))\n",
    "    with tf.control_dependencies([train_mean,train_var]):\n",
    "        return tf.nn.batch_normalization(inputs,mean,var,beta,scale,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "xin = tf.placeholder(tf.float32,shape=[None, 32,32,3])\n",
    "y_ = tf.placeholder(tf.float32,shape=[None, 2])\n",
    "epsilon=0.000001\n",
    "is_training=tf.placeholder(tf.bool,[])\n",
    "\n",
    "x_image = tf.reshape(xin, [-1, 32,32, 3])\n",
    "\n",
    "W1=weight_variable([2,2,3,32],'w1')\n",
    "b1=bias_variable([32],'b2')\n",
    "\n",
    "h_conv1=tf.nn.relu(bn(is_training,conv2d(x_image,W1))+b1)\n",
    "\n",
    "\n",
    "w2=weight_variable([2,2,32,32],'w2')\n",
    "b2=bias_variable([32],'b2')\n",
    "conv1=tf.nn.relu(bn(is_training,conv2d(h_conv1,w2))+b2)\n",
    "conv1=max_pool(conv1)\n",
    "\n",
    "w3=weight_variable([2,2,32,64],'w3')\n",
    "b3=bias_variable([64],'b3')\n",
    "conv2=tf.nn.relu(bn(is_training,conv2d(conv1,w3))+b3)\n",
    "\n",
    "w4=weight_variable([2,2,64,64],'w4')\n",
    "b4=bias_variable([64],'b4')\n",
    "conv3=tf.nn.relu(bn(is_training,conv2d(conv2,w4))+b4)\n",
    "conv3=max_pool(conv3)\n",
    "\n",
    "\n",
    "w5=weight_variable([2,2,64,128],'w5')\n",
    "b5=bias_variable([128],'b5')\n",
    "conv4=tf.nn.relu(bn(is_training,conv2d(conv3,w5))+b5)\n",
    "\n",
    "w6=weight_variable([2,2,128,128],'w6')\n",
    "b6=bias_variable([128],'b6')\n",
    "conv5=tf.nn.relu(bn(is_training,conv2d(conv4,w6))+b6)\n",
    "conv5=max_pool(conv5)\n",
    "\n",
    "w7=weight_variable([2,2,128,256],'w7')\n",
    "b7=bias_variable([256],'b7')\n",
    "conv6=tf.nn.relu(bn(is_training,conv2d(conv5,w7))+b7)\n",
    "\n",
    "w8=weight_variable([2,2,256,256],'w8')\n",
    "b8=bias_variable([256],'b8')\n",
    "conv7=tf.nn.relu(bn(is_training,conv2d(conv6,w8))+b8)\n",
    "#conv6=max_pool(conv6)\n",
    "\n",
    "W_fc1=weight_variable([16*256,1000],'w_fc1')\n",
    "b_fc1=bias_variable([1000],'b_fc1')\n",
    "y1=tf.reshape(conv6,[-1,16*256])\n",
    "y_fc1=tf.nn.relu(tf.matmul(y1,W_fc1)+b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(y_fc1, keep_prob)\n",
    "\n",
    "W_fc2=weight_variable([1000,100],'w_fc2')\n",
    "b_fc2=bias_variable([100],'b_fc2')\n",
    "y_fc2=tf.nn.relu(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)\n",
    "\n",
    "h_fc2_drop = tf.nn.dropout(y_fc2, keep_prob)\n",
    "\n",
    "W_fc3=weight_variable([100,2],'w_fc3')\n",
    "b_fc3=bias_variable([2],'b_fc3')\n",
    "y_fc3=tf.nn.relu(tf.matmul(h_fc2_drop,W_fc3)+b_fc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.argmax(y_fc3, 1)\n",
    "lab = tf.argmax(y_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_fc3))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.cast(tf.equal(tf.argmax(y_fc3, 1), tf.argmax(y_, 1)),tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=os.listdir(r'/home/smst/Desktop/benjamin/train_he_se_bg')\n",
    "r.sort()\n",
    "qt={}\n",
    "for i in range(0,len(r),3): \n",
    "    qt.update({r[i]:bl[i]})\n",
    "    qt.update({r[i+1]:bl[i+1]})\n",
    "    qt.update({r[i+2]:bl[i+2]})\n",
    "    \n",
    "ri=os.listdir(r'../Desktop/benjamin/test_he_se_bg')\n",
    "ri.sort()\n",
    "qte={}\n",
    "for i in range(0,len(ri),3): \n",
    "    qte.update({ri[i]:test_label[i]})\n",
    "    qte.update({ri[i+1]:test_label[i+1]})\n",
    "    qte.update({ri[i+2]:test_label[i+2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('session started')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess,'../soft_and_hard_class_models/model_201.ckpt')\n",
    "    r=os.listdir(r'../train_he_se_bg')\n",
    "    r.sort()\n",
    "    shuffle(r)\n",
    "    no_test=os.listdir(r'../test_he_se_bg')\n",
    "    no_test.sort()\n",
    "    a=datetime.datetime.now().time()\n",
    "    print(a)\n",
    "    for epoch in range(1000):\n",
    "        a=datetime.datetime.now().time()\n",
    "        print('EPOCH %d started at %s'%(epoch+1,a))\n",
    "        for i in range(0,300000,100):\n",
    "            r_lis=r[i:i+100]\n",
    "            for k in r_lis: \n",
    "                a=Image.open(r'../train_he_se_bg/%s'%k)\n",
    "                npg=np.array(a,dtype='f')\n",
    "                nt=np.array([npg])\n",
    "                s=np.array([qt[k]])\n",
    "                if k==r_lis[0]:\n",
    "                    ti=np.array([npg])\n",
    "                    labels=np.array([qt[k]])\n",
    "                else:\n",
    "                    ti = np.append(ti,nt,axis=0)#300\n",
    "                    labels = np.append(labels,s,axis=0) \n",
    "            train_step.run(feed_dict={xin:ti,y_:labels,is_training:True,keep_prob: 0.5})\n",
    "            pre=correct_prediction.eval(feed_dict={xin:ti,y_:labels,is_training:False,keep_prob: 1})\n",
    "            if i==0:\n",
    "                axx=pre\n",
    "            else:\n",
    "                axx=np.append(axx,pre)\n",
    "        train=sum(axx)/len(axx)\n",
    "        print('train accuracy of EPOCH %d is: %f'%((epoch+1),train))\n",
    "        save_path=saver.save(sess,r'../classification_he_se/model_%03d.ckpt'%(epoch+1))        \n",
    "        \n",
    "        for il in range(0,90000,100):\n",
    "            test_lis=no_test[il:il+100]\n",
    "            for k in test_lis: \n",
    "                a=Image.open(r'../test_he_se_bg/%s'%k)\n",
    "                npg=np.array(a,dtype='f')\n",
    "                nt=np.array([npg])\n",
    "                s=np.array([qte[k]])\n",
    "                if k==test_lis[0]:\n",
    "                    ti=np.array([npg])\n",
    "                    labels=np.array([qte[k]])\n",
    "                else:\n",
    "                    ti = np.append(ti,nt,axis=0)#300\n",
    "                    labels = np.append(labels,s,axis=0) \n",
    "            pre=correct_prediction.eval(feed_dict={xin:ti,y_:labels,is_training:False,keep_prob: 1})\n",
    "            if il==0:\n",
    "                axx=pre\n",
    "            else:\n",
    "                axx=np.append(axx,pre)\n",
    "        test=sum(axx)/len(axx)\n",
    "        print('test accuracy of EPOCH %d is: %f'%((epoch+1),test))\n",
    "        if test>0.9:\n",
    "            pr=np.array([])\n",
    "            ded=np.array([])\n",
    "            for il in range(0,90000,100):\n",
    "                test_lis=no_test[il:il+100]\n",
    "                for k in test_lis: \n",
    "                    a=Image.open(r'../test_he_se_bg/%s'%k)\n",
    "                    npg=np.array(a,dtype='f')\n",
    "                    nt=np.array([npg])\n",
    "                    s=np.array([qte[k]])\n",
    "                    if k==test_lis[0]:\n",
    "                        test_i=np.array([npg])\n",
    "                        test_la=np.array([qte[k]])\n",
    "                    else:\n",
    "                        test_i = np.append(test_i,nt,axis=0)#300\n",
    "                        test_la = np.append(test_la,s,axis=0)\n",
    "                pre=pred.eval(feed_dict={xin:test_i,y_:test_la,is_training:False,keep_prob: 1})\n",
    "                pr=np.append(pr,pre,axis=0)\n",
    "                labe=lab.eval(feed_dict={xin:test_i,y_:test_la,is_training:False,keep_prob: 1})\n",
    "                ded=np.append(ded,labe,axis=0)\n",
    "            np.savez(r'../roc/label_%02d.npz'%epoch , ded )\n",
    "            np.savez(r'../roc/predi_%02d.npz'%epoch , pr )\n",
    "        a=datetime.datetime.now().time()\n",
    "        print('EPOCH %d ENDED at %s'%(epoch+1,a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
